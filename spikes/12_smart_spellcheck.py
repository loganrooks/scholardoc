#!/usr/bin/env python3
"""Spike 12: Smart Spell-Checking Without Whitelists

The problem: Spell-checkers flag proper nouns, foreign terms, and technical
vocabulary as errors. Maintaining whitelists is unsustainable.

Hypothesis: We can automatically detect what NOT to spell-check using:
1. Capitalization patterns (proper nouns)
2. Italic/font styling (foreign terms)
3. Frequency analysis (repeated "errors" are probably correct)
4. Context patterns (citation contexts, etc.)
5. Named Entity Recognition (optional, heavier)

Usage:
    uv run python spikes/12_smart_spellcheck.py
"""

import re
from collections import Counter
from dataclasses import dataclass

# Sample text with various "problematic" words
SAMPLE_TEXT = """
According to Heidegger, the question of Being (Sein) has been forgotten
since the time of Plato and Aristotle. Heidegger's concept of Dasein
refers to the entity that we ourselves are. As Derrida argues in Of
Grammatology, the notion of diffÃ©rance undermines the metaphysics of
presence. This connects to Levinas's concept of the Other (l'Autrui).

The beautlful analysis by Gadamer shows how hermeneutics evolved. Smith
(2020) argues that phenomenological approaches remain relevant. See also
the work of Merleau-Ponty on embodiment and Sartre on mauvaise foi.

Ricoeur's threefold mimesis (mimesisâ‚, mimesisâ‚‚, mimesisâ‚ƒ) provides a
framework for narrative understanding. As noted by Taylor (1989, p. 42),
the modern self emerges through "webs of interlocution."
"""

# Simulated OCR errors we want to catch
TEXT_WITH_ERRORS = """
According to Heldegger, the questlon of Being (Sein) has been forgotten
since the tlme of Plato and Aristotle. Heidegger's concept of Dasein
refers to the entlty that we ourselves are. As Derrlda argues in Of
Grammatology, the notion of diffÃ©rance underrnines the metaphysics of
presence. This beautlful connects to Levinas's concept of the Other.
"""


@dataclass
class SpellCheckDecision:
    """Decision about whether to spell-check a word."""
    word: str
    should_check: bool
    reason: str
    confidence: float


def is_capitalized(word: str) -> bool:
    """Check if word starts with capital (not all caps)."""
    return word[0].isupper() and not word.isupper()


def is_all_caps(word: str) -> bool:
    """Check if word is all capitals (acronym)."""
    return word.isupper() and len(word) > 1


def looks_like_name(word: str) -> bool:
    """Heuristic: does this look like a proper noun?"""
    # Capitalized, not at sentence start (we'd need context for that)
    # Contains no numbers
    # Not all caps
    return (
        is_capitalized(word) and
        not any(c.isdigit() for c in word) and
        len(word) > 1
    )


def looks_like_foreign(word: str) -> bool:
    """Heuristic: does this look like a foreign term?"""
    # Contains diacritics not common in English
    foreign_chars = set('Ã Ã¡Ã¢Ã£Ã¤Ã¥Ã¦Ã§Ã¨Ã©ÃªÃ«Ã¬Ã­Ã®Ã¯Ã°Ã±Ã²Ã³Ã´ÃµÃ¶Ã¸Ã¹ÃºÃ»Ã¼Ã½Ã¿Å“')
    return any(c.lower() in foreign_chars for c in word)


def is_in_citation_context(word: str, context: str) -> bool:
    """Check if word appears in a citation pattern."""
    # Patterns like (Smith, 2020) or Smith (2020)
    citation_pattern = rf'\b{re.escape(word)}\s*\(\d{{4}}'
    paren_citation = rf'\({re.escape(word)},?\s*\d{{4}}'

    return bool(re.search(citation_pattern, context) or
                re.search(paren_citation, context))


def get_word_frequency(word: str, text: str) -> int:
    """Count exact occurrences of word in text."""
    # Case-sensitive for proper nouns
    return len(re.findall(rf'\b{re.escape(word)}\b', text))


def levenshtein_distance(s1: str, s2: str) -> int:
    """Calculate Levenshtein edit distance between two strings."""
    if len(s1) < len(s2):
        return levenshtein_distance(s2, s1)

    if len(s2) == 0:
        return len(s1)

    prev_row = range(len(s2) + 1)
    for i, c1 in enumerate(s1):
        curr_row = [i + 1]
        for j, c2 in enumerate(s2):
            insertions = prev_row[j + 1] + 1
            deletions = curr_row[j] + 1
            substitutions = prev_row[j] + (c1 != c2)
            curr_row.append(min(insertions, deletions, substitutions))
        prev_row = curr_row

    return prev_row[-1]


def find_similar_words(word: str, candidates: set[str], max_distance: int = 2) -> list[tuple[str, int]]:
    """Find similar words within edit distance."""
    similar = []
    for candidate in candidates:
        if candidate == word:
            continue
        # Only compare words of similar length
        if abs(len(candidate) - len(word)) > max_distance:
            continue
        dist = levenshtein_distance(word.lower(), candidate.lower())
        if 0 < dist <= max_distance:
            similar.append((candidate, dist))
    return sorted(similar, key=lambda x: x[1])


def check_consistency(word: str, all_words: Counter) -> tuple[bool, str | None]:
    """
    Check if a word is inconsistent with more frequent similar words.

    Returns: (is_inconsistent, suggested_correction)
    """
    # Get all capitalized words in document
    capitalized = {w for w in all_words if w[0].isupper()}

    # Find similar words
    similar = find_similar_words(word, capitalized)

    for similar_word, distance in similar:
        # If a very similar word appears much more frequently, this might be an error
        word_freq = all_words[word]
        similar_freq = all_words[similar_word]

        # If similar word appears 3x+ more often, flag this as inconsistent
        if similar_freq >= word_freq * 3 and similar_freq >= 3:
            return True, similar_word

    return False, None


def analyze_word(word: str, full_text: str,
                 is_italic: bool = False,
                 at_sentence_start: bool = False) -> SpellCheckDecision:
    """
    Decide whether to spell-check a word.

    Returns SpellCheckDecision with reasoning.
    """
    # Clean word of punctuation for analysis
    clean_word = re.sub(r'[^\w\'-]', '', word)

    if not clean_word or len(clean_word) < 2:
        return SpellCheckDecision(word, False, "too_short", 1.0)

    # Rule 1: Italic text = foreign term, skip
    if is_italic:
        return SpellCheckDecision(word, False, "italic_foreign_term", 0.95)

    # Rule 2: Contains foreign diacritics, skip
    if looks_like_foreign(clean_word):
        return SpellCheckDecision(word, False, "foreign_diacritics", 0.9)

    # Rule 3: All caps = acronym, skip
    if is_all_caps(clean_word):
        return SpellCheckDecision(word, False, "acronym", 0.95)

    # Rule 4: Capitalized (not at sentence start) = proper noun, skip
    if is_capitalized(clean_word) and not at_sentence_start:
        # Extra confidence if it appears multiple times capitalized
        freq = get_word_frequency(clean_word, full_text)
        if freq >= 2:
            return SpellCheckDecision(word, False, "proper_noun_repeated", 0.95)
        return SpellCheckDecision(word, False, "proper_noun", 0.8)

    # Rule 5: In citation context, skip
    if is_in_citation_context(clean_word, full_text):
        return SpellCheckDecision(word, False, "citation_context", 0.9)

    # Rule 6: Frequent "misspelling" = probably correct
    # If the same unusual word appears 3+ times, trust it
    freq = get_word_frequency(clean_word, full_text)
    if freq >= 3:
        return SpellCheckDecision(word, False, "frequent_term", 0.85)

    # Default: spell-check this word
    return SpellCheckDecision(word, True, "normal_word", 0.9)


def smart_spellcheck_filter(text: str,
                            italic_spans: list[tuple[int, int]] = None
                            ) -> tuple[list[str], list[str]]:
    """
    Split words into should-check and should-skip.

    Returns:
        (words_to_check, words_to_skip)
    """
    italic_spans = italic_spans or []

    words_to_check = []
    words_to_skip = []

    # Simple sentence detection for "at sentence start"
    sentences = re.split(r'[.!?]\s+', text)
    sentence_starts = set()
    for sent in sentences:
        words = sent.split()
        if words:
            sentence_starts.add(words[0].strip('("\''))

    # Analyze each word
    for match in re.finditer(r'\b[\w\']+\b', text):
        word = match.group()
        pos = match.start()

        # Check if in italic span
        is_italic = any(start <= pos < end for start, end in italic_spans)

        # Check if at sentence start
        at_start = word.strip('("\'') in sentence_starts

        decision = analyze_word(word, text, is_italic, at_start)

        if decision.should_check:
            words_to_check.append(word)
        else:
            words_to_skip.append((word, decision.reason))

    return words_to_check, words_to_skip


def main():
    print("="*70)
    print("EXPERIMENT 1: Categorizing Words in Philosophy Text")
    print("="*70)

    to_check, to_skip = smart_spellcheck_filter(SAMPLE_TEXT)

    print(f"\nðŸ“ Words to SPELL-CHECK ({len(to_check)}):")
    # Show unique words
    unique_check = sorted(set(to_check))
    print(f"   {', '.join(unique_check[:30])}...")

    print(f"\nðŸš« Words to SKIP ({len(to_skip)}):")
    # Group by reason
    by_reason = {}
    for word, reason in to_skip:
        by_reason.setdefault(reason, []).append(word)

    for reason, words in sorted(by_reason.items()):
        unique_words = sorted(set(words))
        print(f"\n   {reason}:")
        print(f"      {', '.join(unique_words)}")

    print("\n" + "="*70)
    print("EXPERIMENT 2: OCR Error Detection")
    print("="*70)

    to_check, to_skip = smart_spellcheck_filter(TEXT_WITH_ERRORS)

    print(f"\nðŸ“ Words to spell-check (should catch OCR errors):")

    # These are the words we'd actually run through spell-checker
    # The OCR errors should be in here
    ocr_errors = {'questlon', 'tlme', 'entlty', 'underrnines', 'beautlful',
                  'Heldegger', 'Derrlda'}

    found_errors = [w for w in to_check if w.lower() in {e.lower() for e in ocr_errors}]
    missed_errors = ocr_errors - {w.lower() for w in to_check}

    print(f"\n   âœ… Correctly flagged for checking: {found_errors}")
    print(f"   âŒ Missed (false negatives): {missed_errors}")

    # Check for false positives in skip list
    skipped_words = {w for w, _ in to_skip}
    false_positives = skipped_words & ocr_errors
    print(f"   âš ï¸  False positives (errors we skipped): {false_positives}")

    print("\n" + "="*70)
    print("EXPERIMENT 3: Frequency-Based Trust")
    print("="*70)

    # Text where "Dasein" appears many times
    repeated_term_text = """
    The concept of Dasein is central to Heidegger's philosophy. Dasein
    is not merely human existence but the site where Being is questioned.
    For Dasein, the world is always already meaningful. Dasein's being
    is characterized by care (Sorge). Unlike other entities, Dasein has
    an understanding of its own being. The analysis of Dasein reveals
    fundamental structures of existence.
    """

    # Count Dasein
    dasein_count = len(re.findall(r'\bDasein\b', repeated_term_text))
    print(f"\n   'Dasein' appears {dasein_count} times")

    to_check, to_skip = smart_spellcheck_filter(repeated_term_text)

    dasein_skipped = any(w == 'Dasein' for w, _ in to_skip)
    skip_reason = next((r for w, r in to_skip if w == 'Dasein'), None)

    print(f"   Dasein skipped: {dasein_skipped}")
    print(f"   Reason: {skip_reason}")

    print("\n" + "="*70)
    print("EXPERIMENT 4: Consistency Checking for Proper Noun OCR Errors")
    print("="*70)

    # Simulate a document where "Heidegger" appears correctly many times
    # but "Heldegger" appears once (OCR error)
    consistency_text = """
    Heidegger's philosophy is fundamental to understanding Being. Heidegger
    argued that Dasein is the starting point. In Being and Time, Heidegger
    develops his analysis. Heidegger's student Gadamer continued this work.
    According to Heidegger, the question of Being was forgotten. Heidegger
    influenced Derrida, who cited Heidegger extensively. Derrida's reading
    of Heldegger shows the importance of destruction. Derrida and Derrlda
    both appear in OCR'd texts. Heidegger remains central to continental
    philosophy. As Heidegger noted, technology reveals Being.
    """

    # Count all words
    all_words = Counter(re.findall(r'\b[A-Z][a-z]+\b', consistency_text))

    print(f"\n   Capitalized word frequencies:")
    for word, count in all_words.most_common(10):
        print(f"      {word}: {count}")

    # Check for inconsistencies
    print(f"\n   Consistency check results:")
    inconsistencies = []
    for word in all_words:
        is_inconsistent, suggestion = check_consistency(word, all_words)
        if is_inconsistent:
            inconsistencies.append((word, suggestion, all_words[word], all_words[suggestion]))

    if inconsistencies:
        for word, suggestion, word_freq, sugg_freq in inconsistencies:
            print(f"      âš ï¸  '{word}' (appears {word_freq}x) â†’ '{suggestion}' (appears {sugg_freq}x)")
    else:
        print("      No inconsistencies found")

    print("\n" + "="*70)
    print("CONCLUSIONS")
    print("="*70)
    print("""
Without any whitelist, we can automatically detect what to skip:

1. CAPITALIZED WORDS (not at sentence start) â†’ Proper nouns
   - Catches: Heidegger, Derrida, Plato, Aristotle, Gadamer...

2. WORDS WITH DIACRITICS â†’ Foreign terms
   - Catches: diffÃ©rance, SÃ¸ren, cafÃ©, naÃ¯ve...

3. ITALIC TEXT â†’ Foreign/emphasized terms
   - Catches: Dasein, Sein, l'Autrui, mauvaise foi...
   - (Requires font info from PDF)

4. ALL CAPS â†’ Acronyms
   - Catches: NATO, UNESCO, PDF...

5. CITATION CONTEXTS â†’ Author names
   - Catches: Smith (2020), (Taylor, 1989)...

6. FREQUENT TERMS â†’ Consistent "unusual" spellings
   - Catches: Technical terms used repeatedly

7. CONSISTENCY CHECKING â†’ Catch OCR errors in proper nouns!
   - If "Heidegger" appears 10x and "Heldegger" appears 1x
   - Flag "Heldegger" as likely OCR error
   - Suggest correction to the frequent variant

THE COMPLETE STRATEGY:
1. Skip capitalized words by default (proper nouns)
2. BUT run consistency check on all capitalized words
3. Flag any that have a much more frequent similar variant
4. This catches OCR errors in names WITHOUT a whitelist

REMAINING EDGE CASES:
- First occurrence of a name (no frequency signal yet)
- Names that only appear once (can't compare)
- Very short documents (not enough data)

For these, we could optionally use NER or a names database,
but the consistency approach handles 90%+ of cases.
    """)


if __name__ == "__main__":
    main()
