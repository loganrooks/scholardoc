# Validation & Testing Methodology

**CRITICAL: Follow these rules to avoid procedural mistakes.**

## Ground Truth Data

- **Location:** `ground_truth/` directory
- **Validation set:** `ground_truth/validation_set.json` (generated by spike 30)
- **Use the FULL validation set**, not just a subset (e.g., 130 pairs, not 30)

## Before Optimizing Anything

1. **Inventory your data** - Know what ground truth exists
2. **Build a proper validation set** - Use `spikes/30_validation_framework.py`
3. **Establish baseline metrics** - Measure BEFORE changing anything
4. **Define success criteria** - What detection rate is acceptable?

## Metrics to Track

- **Detection rate** - % of OCR errors caught (target: >99%)
- **False positive rate** - % of correct words flagged (target: <10%)
- **Re-OCR volume** - % of words sent to expensive neural re-OCR
- **Processing time** - ms per page (for performance optimization)

## Testing Rules

- **Test on the FULL validation set**, not cherry-picked examples
- **Report false negatives** - These are critical failures
- **Report false positives** - These waste resources but aren't critical
- **Measure timing** - We're optimizing for speed too

## Common Mistakes to Avoid

1. ❌ Testing on tiny subsets (30 examples) when thousands exist
2. ❌ Proposing pipelines without actually testing them
3. ❌ Ignoring edge cases found during testing
4. ❌ Not measuring performance/timing impact
5. ❌ Optimizing accuracy without considering resource cost

## Ground Truth Organization

```
ground_truth/
├── ocr_errors/
│   ├── ocr_error_pairs.json    # 30 verified pairs (high quality)
│   ├── validated_samples.json   # Additional validated samples
│   └── challenging_samples.json # Edge cases
├── ocr_quality/
│   ├── classified/             # 172 pages, 17k evidence entries
│   ├── samples/                # Per-document sample reviews
│   └── reviewed/               # Human-reviewed batches
├── footnotes/                   # Footnote ground truth
└── validation_set.json         # COMBINED validation set (use this!)
```

## Validation Workflow

### 1. Ground Truth Creation

Use the annotation workflow for creating verified ground truth:

```bash
# Step 1: Claude annotates (use /project:annotate command or agent)
# Step 2: Human reviews flagged items
uv run python spikes/07_annotation_review.py review <annotations.yaml> --pdf <pdf>
# Step 3: Validate
uv run python spikes/07_annotation_review.py validate <annotations.yaml>
```

### 2. Validation Set Building

```bash
# Build comprehensive validation set from all ground truth
uv run python spikes/30_validation_framework.py
```

### 3. Pipeline Testing

```bash
# Test OCR pipeline against full validation set
uv run python spikes/29_ocr_pipeline_design.py
```

### 4. Metric Reporting

Always report:
- Total pairs tested
- Detection rate (with false negative examples)
- False positive rate (with examples if significant)
- Processing time per page
- Re-OCR volume

## Testing Best Practices

### Do:
- ✅ Use the full validation set for all testing
- ✅ Measure baseline before optimizing
- ✅ Report both accuracy and performance metrics
- ✅ Document edge cases discovered during testing
- ✅ Test on representative PDF samples (academic papers, scanned documents)

### Don't:
- ❌ Cherry-pick examples to make results look better
- ❌ Propose changes without empirical testing
- ❌ Ignore performance impact of accuracy improvements
- ❌ Test only on "easy" documents
- ❌ Skip measuring baseline metrics

## Integration Testing

When integrating OCR pipeline:
1. Test on full validation set (not subsets)
2. Verify no regressions in existing tests
3. Measure performance impact
4. Document any new edge cases
5. Update validation set if new ground truth discovered
